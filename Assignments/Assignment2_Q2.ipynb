{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "iK_SwLsMa1ue"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kHViLitW7rL8"
   },
   "source": [
    "### 3 Layer Feed Forward Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "lGOkRKdsqLlV"
   },
   "outputs": [],
   "source": [
    "class FeedForwardNet(object):\n",
    "  \"\"\"\n",
    "  A Simple 3 Layer Feed Forward Neural Network\n",
    "  \"\"\"\n",
    "  def __init__(self, n_input=None, n_hidden=None, n_output=None, h_activation='tanh'):\n",
    "    self.n_input = n_input\n",
    "    self.n_hidden = n_hidden\n",
    "    self.n_output = n_output\n",
    "    \n",
    "    self.W_h = 2*np.random.randn(self.n_input, self.n_hidden)-1 # Initial weights between Input and Hidden Layer\n",
    "    self.b_h = np.zeros(self.n_hidden)  # Initial bias between Input and Hidden Layer\n",
    "    self.W_o = 2*np.random.randn(self.n_hidden, self.n_output)-1 # Initial weights between Hidden and Output Layer\n",
    "    self.b_o = np.zeros(self.n_output)# Initial weights between Hidden and Output Layer\n",
    "    \n",
    "    # Computed during forward pass\n",
    "    self.z_h = None # Hidden layer linear output\n",
    "    self.a_h = None # Hidden layer activation \n",
    "    self.z_o = None # Final layer linear output\n",
    "    self.a_o = None # Final layer activation\n",
    "    \n",
    "    #Computed during backward pass\n",
    "    self.dW_h = None # Hidden Layer Weight Gradients\n",
    "    self.db_h = None # Hidden Layer Bias Gradients\n",
    "    self.dW_o = None # Output Layer Weight Gradients\n",
    "    self.db_o = None # Output Layer Bias Gradients\n",
    "    \n",
    "    self.h_activation = h_activation\n",
    "    self.history = []\n",
    "    \n",
    "  def predict(self, X):\n",
    "    probs = self.forward(X)\n",
    "    return np.argmax(probs[0], axis=1)\n",
    "  \n",
    "  def forward(self, X):\n",
    "    self.z_h = X.dot(self.W_h) + self.b_h # Hidden Layer Output\n",
    "    self.a_h = self.hidden_layer_activation(self.z_h) # Hidden Layer Activations\n",
    "    self.z_o = self.a_h.dot(self.W_o) + self.b_o # Final Layer Output\n",
    "    self.a_o = self.softmax(self.z_o) # Final Layer Activations\n",
    "    \n",
    "    probs = self.a_o\n",
    "    return probs\n",
    "    \n",
    "  def backprop(self, X, y):\n",
    "    probs = self.a_o\n",
    "    dL_o = self.cross_entropy_derivative(probs, y)\n",
    "    self.dW_o = (self.a_h.T).dot(dL_o)\n",
    "    self.db_o = np.sum(dL_o, axis=0)\n",
    "    \n",
    "    dL_h = dL_o.dot(self.W_o.T) * self.hidden_layer_activation_derivative(self.z_h)\n",
    "    self.dW_h = np.dot(X.T, dL_h)\n",
    "    self.db_h = np.sum(dL_h, axis=0)\n",
    "    \n",
    "\n",
    "  def softmax(self, x):\n",
    "    scores = np.exp(x - np.max(x)) # For numerical stability\n",
    "    probs = scores / np.sum(scores, axis=1, keepdims=True)\n",
    "    return probs \n",
    "  \n",
    "  def hidden_layer_activation(self, x):\n",
    "    if self.h_activation == 'relu':\n",
    "      return self.relu(x)\n",
    "    elif self.h_activation == 'tanh':\n",
    "      return self.tanh(x)\n",
    "    elif self.h_activation == 'sigmoid':\n",
    "      return self.sigmoid(x)\n",
    "    else:\n",
    "      raise NotImplementedError\n",
    "      \n",
    "  def hidden_layer_activation_derivative(self, x):\n",
    "    if self.h_activation == 'relu':\n",
    "      return self.relu_derivative(x)\n",
    "    elif self.h_activation == 'tanh':\n",
    "      return self.tanh_derivative(x)\n",
    "    elif self.h_activation == 'sigmoid':\n",
    "      return self.sigmoid_derivative(x)\n",
    "    else:\n",
    "      raise NotImplementedError\n",
    "  \n",
    "  def tanh(self, x):\n",
    "    return np.tanh(x)\n",
    "  \n",
    "  def tanh_derivative(self, x):\n",
    "    return (1 - np.power(self.tanh(x), 2))\n",
    "  \n",
    "  def sigmoid(self, x):\n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "  def sigmoid_derivative(self, x):\n",
    "    return x * (1. - x)\n",
    "    \n",
    "  def relu(self, x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "  def relu_derivative(self, x):\n",
    "    return 1 * (x > 0)\n",
    "  \n",
    "  def cross_entropy_loss(self, probs, y):\n",
    "    num_of_examples = y.shape[0]\n",
    "    log_likelihood = -np.log(probs[range(num_of_examples),y])\n",
    "    loss = np.sum(log_likelihood) / num_of_examples\n",
    "    return loss\n",
    "\n",
    "  def cross_entropy_derivative(self, probs, y):\n",
    "    num_of_examples = y.shape[0]\n",
    "    probs[range(num_of_examples),y] -= 1\n",
    "    return probs\n",
    "    \n",
    "  def train(self, X_train, y_train, learning_rate=0.01, epochs=1000, verbose=0):\n",
    "    \n",
    "    # Vanilla Gradient Descent Update\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Forward Propagation\n",
    "        probs = self.forward(X_train)\n",
    "        loss = self.cross_entropy_loss(probs,y_train)\n",
    "        \n",
    "        # Backward Propagation\n",
    "        self.backprop(X_train, y_train)\n",
    "        \n",
    "        # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "        self.dW_o += 0.1 * self.W_o\n",
    "        self.dW_h += 0.1 * self.W_h\n",
    "       \n",
    "        # Gradient Descent Parameter Updates\n",
    "        self.W_o += -learning_rate * self.dW_o\n",
    "        self.b_o += -learning_rate * self.db_o\n",
    "        self.W_h += -learning_rate * self.dW_h\n",
    "        self.b_h += -learning_rate * self.db_h\n",
    "         \n",
    "        # Print loss\n",
    "        if verbose==0 and i % 1000 == 0:\n",
    "          print(\"Loss after epoch {} {}\".format(i, loss))\n",
    "        self.history.append(loss)\n",
    "     \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuho6v-nc5tQ"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "8ycHaC-xg69S"
   },
   "outputs": [],
   "source": [
    "# Download the datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "uris = [\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/dermatology/dermatology.data',\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/pendigits/pendigits.tra' ]\n",
    "dermatology_dataset, pendigit_dataset = [pd.read_csv(uri, header=None) for uri in uris]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 733,
     "status": "ok",
     "timestamp": 1523184812483,
     "user": {
      "displayName": "Naveen Pandey",
      "photoUrl": "//lh3.googleusercontent.com/-A5U04NEDO80/AAAAAAAAAAI/AAAAAAAABAs/JwZT5UFMBu0/s50-c-k-no/photo.jpg",
      "userId": "110479253344487916602"
     },
     "user_tz": -330
    },
    "id": "YKdSD-Zg6mRB",
    "outputId": "4d07017e-755a-4aaa-86ca-35856f858462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dermatology Dataset Shape : (366, 35)\n",
      "   0   1   2   3   4   5   6   7   8   9  ...  25  26  27  28  29  30  31  32  \\\n",
      "0   2   2   0   3   0   0   0   0   1   0 ...   0   0   3   0   0   0   1   0   \n",
      "1   3   3   3   2   1   0   0   0   1   1 ...   0   0   0   0   0   0   1   0   \n",
      "2   2   1   2   3   1   3   0   3   0   0 ...   0   2   3   2   0   0   2   3   \n",
      "3   2   2   2   0   0   0   0   0   3   2 ...   3   0   0   0   0   0   3   0   \n",
      "4   2   3   2   2   2   2   0   2   0   0 ...   2   3   2   3   0   0   2   3   \n",
      "\n",
      "   33  34  \n",
      "0  55   2  \n",
      "1   8   1  \n",
      "2  26   3  \n",
      "3  40   1  \n",
      "4  45   3  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "Dermatology Dataset Shape After Cleanup : (358, 35)\n",
      "Features Shape: (242, 34)\n",
      "Labels Shape: (242,)\n"
     ]
    }
   ],
   "source": [
    "#Dermatology Dataset Exploration\n",
    "print('Dermatology Dataset Shape : {}'.format(dermatology_dataset.shape))\n",
    "print(dermatology_dataset.head(5))\n",
    "\n",
    "# Remove rows with missing values\n",
    "dermatology_dataset.iloc[:,33] = pd.to_numeric(dermatology_dataset.iloc[:,33], errors='coerce')\n",
    "dermatology_dataset = dermatology_dataset.dropna()\n",
    "print('Dermatology Dataset Shape After Cleanup : {}'.format(dermatology_dataset.shape))\n",
    "\n",
    "# Filter out the data for 3 classes from the dataset\n",
    "dm_class_labels = {1:'psoriasis',2:'seboreic dermatitis', 3:'lichen planus'}\n",
    "dermatology_dataset = dermatology_dataset.loc[dermatology_dataset.iloc[:,34].isin(dm_class_labels.keys())]\n",
    "\n",
    "X_dm, y_dm = dermatology_dataset.iloc[:,0:dermatology_dataset.shape[1]-1], dermatology_dataset.iloc[:,-1]\n",
    "X_dm, y_dm = X_dm.as_matrix() , y_dm.as_matrix()\n",
    "\n",
    "# Normalise the features for faster convergence \n",
    "#X_dm = (X_dm - np.mean(X_dm, axis=0)) / np.std(X_dm, axis=0)\n",
    "# Convert labels to 0,1,2 for easier processing\n",
    "y_dm -= 1\n",
    "\n",
    "print('Features Shape: {}'.format(X_dm.shape))\n",
    "print('Labels Shape: {}'.format(y_dm.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 907,
     "status": "ok",
     "timestamp": 1523184814510,
     "user": {
      "displayName": "Naveen Pandey",
      "photoUrl": "//lh3.googleusercontent.com/-A5U04NEDO80/AAAAAAAAAAI/AAAAAAAABAs/JwZT5UFMBu0/s50-c-k-no/photo.jpg",
      "userId": "110479253344487916602"
     },
     "user_tz": -330
    },
    "id": "Hx2BRQ-26oqZ",
    "outputId": "2e530944-b106-46c2-949a-e89ef172fa66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pen Digit Dataset Shape : (7494, 17)\n",
      "   0    1   2    3    4    5    6    7   8   9    10  11   12  13   14  15  16\n",
      "0  47  100  27   81   57   37   26    0   0  23   56  53  100  90   40  98   8\n",
      "1   0   89  27  100   42   75   29   45  15  15   37   0   69   2  100   6   2\n",
      "2   0   57  31   68   72   90  100  100  76  75   50  51   28  25   16   0   1\n",
      "3   0  100   7   92    5   68   19   45  86  34  100  45   74  23   67   0   4\n",
      "4   0   67  49   83  100  100   81   80  60  60   40  40   33  20   47   0   1\n",
      "Features Shape: (3058, 16)\n",
      "Labels Shape: (3058,)\n"
     ]
    }
   ],
   "source": [
    "#Pen Digits Dataset Exploration\n",
    "print('Pen Digit Dataset Shape : {}'.format(pendigit_dataset.shape)) \n",
    "print(pendigit_dataset.head(5))\n",
    "\n",
    "# Filter out the data for 4 digits from the dataset\n",
    "pendigit_dataset = pendigit_dataset.loc[pendigit_dataset.iloc[:,16].isin([0,1,2,3])]\n",
    "\n",
    "X_pd, y_pd = pendigit_dataset.iloc[:,0:16], pendigit_dataset.iloc[:,16]\n",
    "X_pd, y_pd = X_pd.as_matrix() , y_pd.as_matrix()\n",
    "\n",
    "# Normalise the features for faster convergence\n",
    "#X_pd = (X_pd - np.mean(X_pd, axis=0)) / np.std(X_pd, axis=0)\n",
    "\n",
    "print('Features Shape: {}'.format(X_pd.shape))\n",
    "print('Labels Shape: {}'.format(y_pd.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "pOCBjGqg6u_4"
   },
   "outputs": [],
   "source": [
    "def generate_k_folds(dataset, k): \n",
    "  \"\"\"\n",
    "  Returns a list of folds, where each fold is a tuple like (training_set, \n",
    "  test_set), where each set is a tuple like (examples, classes)\n",
    "  \"\"\"\n",
    "  folds=[]\n",
    "  n=dataset[0].shape[0]\n",
    "  fold_size = n//k\n",
    "  # Divide the data into k equal subsections, keep k-1 section for training\n",
    "  # and 1 for testing, repeat k times to generate folds\n",
    "  for i in range(k):\n",
    "    indices = [j for j in range(n)]\n",
    "    if i == k-1:\n",
    "      fold_size = n - i*fold_size\n",
    "    test_idx = indices[i*fold_size:i*fold_size+fold_size],\n",
    "    training_idx = indices[0:i*fold_size] + indices[i*fold_size+fold_size:]\n",
    "    \n",
    "    examples=dataset[0]\n",
    "    classes=dataset[1]\n",
    "    training_set_examples=examples[training_idx,:]\n",
    "    training_set_classes=np.array(classes)[training_idx]\n",
    "    training_set=(training_set_examples,training_set_classes)\n",
    "    test_set_examples=examples[test_idx,:]\n",
    "    test_set_classes=np.array(classes)[test_idx]\n",
    "    test_set=(test_set_examples,test_set_classes)\n",
    "    fold =(training_set,test_set)\n",
    "    folds.append(fold)\n",
    "  return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "9B2huNLF69Gr"
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_validation_accuracy(folds, epochs, learning_rate, n_hidden, h_activation): \n",
    "  \"\"\"Trains the model and returns its k-fold cross validation accuracy for specified params\"\"\"\n",
    "  scores = []\n",
    "  for i, fold in enumerate(folds):\n",
    "    train, valid = fold\n",
    "    X_valid, y_valid = valid\n",
    "    X_train, y_train = train\n",
    "    \n",
    "    n_classes = len(set(y_train))\n",
    "    model = FeedForwardNet(n_input=X_train.shape[1], \\\n",
    "                           n_hidden=n_hidden, \\\n",
    "                           n_output=n_classes, \\\n",
    "                           h_activation=h_activation )\n",
    "    model.train(X_train, \n",
    "                y_train, \n",
    "                epochs=epochs, \n",
    "                learning_rate=learning_rate, \n",
    "                verbose=1)\n",
    "    \n",
    "    y_pred = model.predict(X_valid)\n",
    "    accuracy = np.mean(y_pred == y_valid)\n",
    "    scores.append(accuracy)\n",
    "  \n",
    "  k_fold_cross_validation_accuracy = 0\n",
    "  if len(scores) > 0:\n",
    "    k_fold_cross_validation_accuracy = sum(scores)/len(scores) \n",
    "  return k_fold_cross_validation_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CD_KbgIE_zAB"
   },
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZ8edCiKSWpk"
   },
   "source": [
    "#### Setup\n",
    "\n",
    "1) No of hidden units: [1, 3, 5, 10, 50, 100]\n",
    "\n",
    "2) Activations: \n",
    "\n",
    "- **tanh**\n",
    "\n",
    "> It squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. \n",
    "\n",
    "\n",
    "- **relu**\n",
    "\n",
    "> It computes the function f(x)=max(0,x). In other words, the activation is simply thresholded at zero (see image above on the left). It has been found to greatly accelerate (e.g. a factor of 6 in Krizhevsky et al.) the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form.Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.\n",
    "\n",
    "4) Epochs: 20000\n",
    "\n",
    "> Instead of choosing threshold values as stopping criteria, I have chosen number of epochs as a stopping criteria. A fairly large number of epochs would be helpful in seeing overfitting patterns and would ensure we are not treating a local minima an arbitrary stopping criteria, hence leading to better generalisation.\n",
    "\n",
    "5) Learning Rate: 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "herhV3oAXVHK"
   },
   "source": [
    "#### Compute 5-folds cross validation accuray for pen digits datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "IsdqgzeIjUcJ",
    "outputId": "616e0061-9e0c-4496-f50e-6386f2caac3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Number of hidden units', 'Hidden Layer Activation', '5-fold CV accuracy']\n",
      "[1, 'tanh', 0.2511445389143231]\n",
      "[3, 'tanh', 0.49411379986919557]\n",
      "[5, 'tanh', 0.38260300850228907]\n",
      "[10, 'tanh', 0.6007194244604317]\n",
      "[50, 'tanh', 0.8776978417266187]\n",
      "[100, 'tanh', 0.7190974493132767]\n",
      "[1, 'relu', 0.2553956834532374]\n",
      "[3, 'relu', 0.25474166121648134]\n",
      "[5, 'relu', 0.25474166121648134]\n",
      "[10, 'relu', 0.25310660562459125]\n",
      "[50, 'relu', 0.2550686723348594]\n"
     ]
    }
   ],
   "source": [
    "hidden_units = [1,3,5,10,50,100]\n",
    "activations = ['tanh', 'relu']\n",
    "epochs = 10000\n",
    "learning_rate = 0.0001\n",
    "pd_folds = generate_k_folds([X_pd, y_pd], 2)\n",
    "pd_results = []\n",
    "print(['Number of hidden units', 'Hidden Layer Activation', '5-fold CV accuracy'])\n",
    "for h_activation in activations:\n",
    "  for n_hidden in hidden_units: \n",
    "    accuracy = k_fold_cross_validation_accuracy(pd_folds, epochs, learning_rate, n_hidden, h_activation)\n",
    "    result = [n_hidden, h_activation, accuracy]\n",
    "    pd_results.append(result)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYa3Hx65XcEF"
   },
   "source": [
    "#### Compute 5-folds cross validation accuray for dermatology datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 192676,
     "status": "ok",
     "timestamp": 1523185362312,
     "user": {
      "displayName": "Naveen Pandey",
      "photoUrl": "//lh3.googleusercontent.com/-A5U04NEDO80/AAAAAAAAAAI/AAAAAAAABAs/JwZT5UFMBu0/s50-c-k-no/photo.jpg",
      "userId": "110479253344487916602"
     },
     "user_tz": -330
    },
    "id": "v4EVowKvPHhf",
    "outputId": "c0b888f9-0acd-4447-c547-01cc40d5e1ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Number of hidden units', 'Hidden Layer Activation', '5-fold CV accuracy']\n",
      "[1, 'tanh', 0.5371900826446281]\n",
      "[3, 'tanh', 0.6487603305785123]\n",
      "[5, 'tanh', 0.8471074380165289]\n",
      "[10, 'tanh', 0.9958677685950413]\n",
      "[50, 'tanh', 0.9917355371900827]\n",
      "[100, 'tanh', 0.9917355371900827]\n",
      "[1, 'relu', 0.22727272727272727]\n",
      "[3, 'relu', 0.22727272727272727]\n",
      "[5, 'relu', 0.22727272727272727]\n",
      "[10, 'relu', 0.41735537190082644]\n",
      "[50, 'relu', 0.45867768595041325]\n",
      "[100, 'relu', 0.45867768595041325]\n"
     ]
    }
   ],
   "source": [
    "# Compute 5-folds cross validation accuray for dermatology datasets\n",
    "dm_folds = generate_k_folds([X_dm, y_dm], 2)\n",
    "hidden_units = [1,3,5,10,50,100]\n",
    "activations = ['tanh','relu']\n",
    "epochs = 20000\n",
    "learning_rate = 0.001\n",
    "\n",
    "dm_results = []\n",
    "print(['Number of hidden units', 'Hidden Layer Activation', '5-fold CV accuracy'])\n",
    "for h_activation in activations:\n",
    "  for n_hidden in hidden_units: \n",
    "    accuracy = k_fold_cross_validation_accuracy(dm_folds, epochs, learning_rate, n_hidden, h_activation)\n",
    "    result = [n_hidden, h_activation, accuracy]\n",
    "    dm_results.append(result)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOpf1o4fIKIz"
   },
   "source": [
    "#### Observations\n",
    "\n",
    "- As we can observe from accuracy tables of both trained datasets, increasing the number of hidden units results in more complex models. When the dataset itself is complex i.e have a large number of distinct features, higher dimensional hidden units help in modeling more complex behaviours and result in better accuracy. On the other hand, in simpler datasets, higher dimensional hidden units are prone to overfitting . It lead to memorisation of the training set which performs poorly on the test set.\n",
    "\n",
    "- For both the datasets, tanh activation function performs better than the relu activation with the given learning rates. It appears a large gradient flowing through the ReLU neuron is causing the weights to update in such a way that the neuron is never activating from that datapoint again. Due to this, the gradient flowing through the unit will is forever be zero from that point on, resulting in consistent loss for large learning rates. Tuning the learning rates and setting it to a lesser values seems to be resolving the issue.\n",
    "\n",
    "- Training is much faster for relu than tanh activation function"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "SMAI_Assignment2_Q2.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
